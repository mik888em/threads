# Threads Metrics

Инструмент для сбора метрик постов Threads и загрузки их в Google Sheets.

## Возможности

- Асинхронно получает посты Threads по списку аккаунтов из листа `accounts_threads`, повторно используя сохранённые курсоры пагинации, чтобы не скачивать старые записи заново.
- Запрашивает Insights только для тех постов, чьи данные устарели, и хранит отметки времени обновления в локальном состоянии.
- Объединяет «сырые» данные постов и Insights, дополняя таблицу метрик без потери уже записанных колонок и обновляя строки по ключу `account_name + post_id` без дублирования.
- После каждой выгрузки отключает перенос текста в диапазоне и принудительно устанавливает высоту строк в 21 пиксель, чтобы таблица оставалась компактной.
- Ведёт JSON-логирование с heartbeat-сообщениями и автоматически останавливается по таймауту, чтобы не зависать бесконечно.

## Требования

- Python 3.11 или новее.
- Учётные данные сервисного аккаунта Google с правами на чтение/запись в нужную таблицу.
- Доступ к Threads Graph API и рабочие токены аккаунтов Threads.

## Установка

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
export PYTHONPATH=src
```

## Конфигурация

Заполните файл `.env` на основе `.env.example`. Критичные переменные:

- `ID_GOOGLE_TABLE` — идентификатор Google-таблицы с данными.
- `URL_GAS_RAZVERTIVANIA` — URL развёрнутого Google Apps Script.
- `GOOGLE_SERVICE_ACCOUNT_JSON` — JSON-ключ сервисного аккаунта Google.
- `THREADS_API_BASE_URL` — базовый URL Threads Graph API (по умолчанию `https://graph.threads.net`, запросы выполняются к версии API `v1.0`).
- `THREADS_REQUEST_TIMEOUT` — таймаут HTTP-запросов в секундах.
- `THREADS_CONCURRENCY` — максимальное число параллельных запросов.
- `THREADS_STATE_FILE` — путь к файлу состояния.
- `THREADS_METRICS_TTL_MIN` — TTL метрик в минутах.
- `THREADS_RUN_TIMEOUT_MIN` — максимальная продолжительность работы сервиса в минутах.

Остальные параметры можно оставить по умолчанию: запросы к API выполняются с таймаутом `THREADS_REQUEST_TIMEOUT`, а параллелизм ограничивается `THREADS_CONCURRENCY`, чтобы не превысить лимиты Threads Graph API.

Приложение не читает `.env` автоматически — переменные должны попасть в окружение перед запуском. В POSIX-оболочках можно сделать так:

```bash
set -a
source .env
set +a
python -m threads_metrics.main run
```

На Windows можно воспользоваться `python-dotenv` (`python -m dotenv run -- python -m threads_metrics.main run`) либо задать переменные вручную через `set KEY=VALUE` перед запуском.

### Что хранят переменные окружения

- `THREADS_STATE_FILE` задаёт путь к JSON-файлу, где сервис кеширует курсоры пагинации и отметки о последнем обновлении метрик.
- `THREADS_METRICS_TTL_MIN` определяет, спустя сколько минут метрики считаются устаревшими и требуется новый сбор.
- `THREADS_CONCURRENCY` ограничивает количество одновременных запросов к Threads Graph API и тем самым защищает от rate limit.

## Структура проекта

- `threads_metrics.main` — CLI-обёртка, конфигурирование зависимостей и управление жизненным циклом сервиса (heartbeat, обработка сигналов, таймаут).
- `threads_metrics.threads_client` — асинхронный клиент Threads Graph API с ретраями на `httpx` и семафором для ограничения параллелизма.
- `threads_metrics.google_sheets` — чтение токенов и запись агрегированных метрик в Google Sheets, синхронизация с локальным состоянием.
- `threads_metrics.state_store` — файловое хранилище курсоров и отметок времени, применяемое для дедупликации и TTL метрик.
- `threads_metrics.aggregation` — объединение данных о постах и их Insights перед выгрузкой.

Логирование ведётся в JSON-формате (`ts`, `level`, `msg`, `context`), поэтому сбор логов интегрируется с системами наблюдаемости без дополнительной обработки.

## Запуск

```bash
python -m threads_metrics.main run
```

Перед выполнением убедитесь, что переменные окружения загружены согласно разделу «Конфигурация».

Команда запуска организует асинхронный сбор постов из Threads по токенам с листа `accounts_threads`,
агрегирует метрики и записывает их на лист `Data_Po_kagdomy_posty`. Прогресс обработки хранится в
файле состояния, а логи выводятся в формате JSON с heartbeat-сообщениями.

### Как это работает

1. **Загрузка конфигурации.** Переменные окружения проверяются и валидируются перед запуском; при ошибке сервис завершится с понятным сообщением.
2. **Чтение токенов и курсоров.** Для каждого аккаунта берётся сохранённый курсор пагинации, чтобы продолжить с места предыдущего запуска.
3. **Сбор постов.** Параллельные запросы ограничены семафором, данные запрашиваются по эндпоинту `https://graph.threads.net/v1.0/me/threads?fields=id,permalink,text,media_type,media_url,like_count,repost_count,reply_count`, а курсор обновляется только после успешного шага.
4. **Сбор Insights.** Для каждого поста проверяется TTL; свежие записи пропускаются, а метрики запрашиваются по эндпоинту `https://graph.threads.net/v1.0/{post_id}/insights?metric=views,likes,replies,reposts,quotes,shares`, что заметно сокращает число запросов к API.
5. **Запись в Google Sheets.** Данные сливаются с текущей таблицей: строки ищутся по сочетанию `account_name` и `post_id`, числовые метрики обновляются без создания дублей, новые колонки добавляются автоматически, а отметка `updated_at` обновляется при каждой выгрузке. После записи лист форматируется — перенос текста отключается, а высота строк принудительно выставляется в 21 пиксель.

### Интеграция с GitHub Actions

- Расписание (`*/10 * * * *`) запускает сбор каждые 10 минут, при этом блок `concurrency` гарантирует, что одновременно выполняется только один экземпляр workflow — новое срабатывание дождётся завершения предыдущего.
- Джобу ограничивает внешний таймаут в 100 минут, а сам сервис завершится раньше (по умолчанию через 100 минут ожидания, значение настраивается через `THREADS_RUN_TIMEOUT_MIN`), если зависнет или получит сигнал остановки.
- Для локального запуска используется та же команда `python -m threads_metrics.main run`, поэтому отладка и CI ведут себя одинаково.

## Тесты

```bash
pytest
```
